{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTF5jQLn4kLv+L9ZNaTLvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sina-salmanpour/False_memory_PTSD/blob/main/PTSD_false_memory2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import mannwhitneyu\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "from statsmodels.stats.power import FTestAnovaPower\n",
        "import os\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 200)"
      ],
      "metadata": {
        "id": "u1HoCj5y4uHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "7EuJGGoc_e6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILES = {\n",
        "    'verbal_PSTD': '/content/drive/MyDrive/AmirFarhang/ptsd_verbal.xlsx',\n",
        "    'verbal_Non-PTSD': '/content/drive/MyDrive/AmirFarhang/non_ptsd_verbal.xlsx',\n",
        "    'verbal_Control': '/content/drive/MyDrive/AmirFarhang/control_verbal.xlsx',\n",
        "    'video_PSTD': '/content/drive/MyDrive/AmirFarhang/ptsd_video.xlsx',\n",
        "    'video_Non-PTSD': '/content/drive/MyDrive/AmirFarhang/non_ptsd_visual.xlsx',\n",
        "    'video_Control': '/content/drive/MyDrive/AmirFarhang/control_video.xlsx'\n",
        "}\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/AmirFarhang/outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "ALPHA = 0.05\n",
        "EFFECT_SIZE_MEDIUM = 0.25  # Cohen's f for ANOVA"
      ],
      "metadata": {
        "id": "_sXubsiH_cz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 2. DATA PROCESSING FUNCTIONS\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "j_GaKtDZ_jb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, file_key):\n",
        "    \"\"\"Load and process individual Excel file into long format.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"⚠️ File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📂 Processing: {file_key}\")\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Remove Subject column if exists\n",
        "    if 'Subject' in df.columns:\n",
        "        df = df.drop(columns=['Subject'])\n",
        "\n",
        "    task, group = file_key.split('_')\n",
        "\n",
        "    # Identify subject columns\n",
        "    subject_cols = [col for col in df.columns if col.lower().startswith('sub')]\n",
        "    id_vars = [col for col in df.columns if col not in subject_cols]\n",
        "\n",
        "    # Melt to long format\n",
        "    df_long = pd.melt(df, id_vars=id_vars, value_vars=subject_cols,\n",
        "                      var_name='candidate', value_name='value')\n",
        "\n",
        "    df_long['group'] = group\n",
        "    df_long['task'] = task\n",
        "\n",
        "    # Create unique subject IDs (prevent collision across groups)\n",
        "    df_long['candidate'] = df_long['group'] + '_' + df_long['candidate'].astype(str)\n",
        "\n",
        "    # Map item types\n",
        "    if 'answer' in df_long.columns:\n",
        "        df_long['item_type'] = df_long['answer'].map({\n",
        "            1: 'Studied',\n",
        "            'CL': 'Lure',\n",
        "            0: 'New'\n",
        "        }).fillna('Unknown')\n",
        "    else:\n",
        "        df_long['item_type'] = 'Unknown'\n",
        "\n",
        "    # Handle emotion column (fix U -> NT)\n",
        "    if 'emotion' in df_long.columns:\n",
        "        df_long['emotion'] = df_long['emotion'].replace('U', 'NT')\n",
        "    else:\n",
        "        df_long['emotion'] = 'Unknown'\n",
        "\n",
        "    # For video task, assign emotions by row blocks if needed\n",
        "    if task == 'video' and df_long['emotion'].eq('Unknown').all():\n",
        "        df_long = df_long.reset_index(drop=True)\n",
        "        df_long['emotion'] = np.select(\n",
        "            [df_long.index < 26, df_long.index < 51, df_long.index < 76],\n",
        "            ['O', 'NT', 'P'],\n",
        "            default='N'\n",
        "        )\n",
        "\n",
        "    # Extract response and RT\n",
        "    def extract_response_rt(val):\n",
        "        if pd.isna(val):\n",
        "            return np.nan, np.nan\n",
        "        try:\n",
        "            val = float(val)\n",
        "        except (ValueError, TypeError):\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        if val > 0:\n",
        "            return 1.0, val\n",
        "        elif val == 0:\n",
        "            return 0.0, np.nan\n",
        "        else:  # negative\n",
        "            return 1.0, np.nan\n",
        "\n",
        "    df_long[['response', 'rt']] = df_long.apply(\n",
        "        lambda row: extract_response_rt(row['value']),\n",
        "        axis=1,\n",
        "        result_type='expand'\n",
        "    )\n",
        "\n",
        "    df_long = df_long.drop(columns=['value'])\n",
        "\n",
        "    # Save modified file\n",
        "    modified_path = os.path.join(OUTPUT_DIR, f\"{file_key}_modified.xlsx\")\n",
        "    df_long.to_excel(modified_path, index=False)\n",
        "    print(f\"✅ Saved: {modified_path}\")\n",
        "\n",
        "    return df_long\n",
        "\n",
        "\n",
        "def impute_negative_rt(df):\n",
        "    \"\"\"Impute negative RT values with cell means (group/task/emotion/item).\"\"\"\n",
        "    print(\"\\n🔧 Imputing negative RT values...\")\n",
        "\n",
        "    # Flag negatives\n",
        "    df['is_negative'] = df['rt'].isna() & (df['response'] == 1)\n",
        "    n_negative = df['is_negative'].sum()\n",
        "    print(f\"   Found {n_negative} negative/missing RTs for 'yes' responses ({n_negative/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Calculate means from valid positive RTs (yes responses only)\n",
        "    positive_means = df[(df['rt'] > 0) & (df['response'] == 1)].groupby(\n",
        "        ['group', 'task', 'emotion', 'item_type']\n",
        "    )['rt'].mean().reset_index(name='mean_rt')\n",
        "\n",
        "    # Merge and impute\n",
        "    df = df.merge(positive_means, on=['group', 'task', 'emotion', 'item_type'], how='left')\n",
        "    df.loc[df['is_negative'], 'rt'] = df.loc[df['is_negative'], 'mean_rt']\n",
        "\n",
        "    # Trim RT to valid range (200-5000ms)\n",
        "    df['rt'] = df['rt'].clip(200, 5000)\n",
        "\n",
        "    df = df.drop(columns=['mean_rt', 'is_negative'])\n",
        "    print(\"✅ Imputation complete\\n\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "u3Sx6Ll5_maO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 3. AGGREGATION (FIXED VERSION)\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "LUwS563W_p-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_data(task_df, task_name):\n",
        "    \"\"\"\n",
        "    Aggregate trial-level data to subject level.\n",
        "    FIXED: Proper filtering within grouped data.\n",
        "    \"\"\"\n",
        "    print(f\"📊 Aggregating {task_name} data...\")\n",
        "\n",
        "    def calc_metrics(group):\n",
        "        \"\"\"Calculate metrics for each subject/emotion/item_type group.\"\"\"\n",
        "        # Filter current group data\n",
        "        studied_data = group[group['item_type'] == 'Studied']\n",
        "        lure_data = group[group['item_type'] == 'Lure']\n",
        "        new_data = group[group['item_type'] == 'New']\n",
        "\n",
        "        # Calculate rates (only for 'yes' responses)\n",
        "        hit_rate = studied_data['response'].mean() if len(studied_data) > 0 else np.nan\n",
        "        false_memory_rate = lure_data['response'].mean() if len(lure_data) > 0 else np.nan\n",
        "        false_alarm_rate = new_data['response'].mean() if len(new_data) > 0 else np.nan\n",
        "\n",
        "        # Calculate RT (only for 'yes' responses)\n",
        "        yes_responses = group[group['response'] == 1]\n",
        "        rt_mean = yes_responses['rt'].mean() if len(yes_responses) > 0 else np.nan\n",
        "        rt_sd = yes_responses['rt'].std() if len(yes_responses) > 0 else np.nan\n",
        "\n",
        "        return pd.Series({\n",
        "            'n_trials': len(group),\n",
        "            'hit_rate': hit_rate,\n",
        "            'false_memory_rate': false_memory_rate,\n",
        "            'false_alarm_rate': false_alarm_rate,\n",
        "            'rt_mean': rt_mean,\n",
        "            'rt_sd': rt_sd\n",
        "        })\n",
        "\n",
        "    agg_df = task_df.groupby(['candidate', 'group', 'emotion', 'item_type']).apply(calc_metrics).reset_index()\n",
        "\n",
        "    # Save\n",
        "    agg_path = os.path.join(OUTPUT_DIR, f'aggregated_{task_name}.xlsx')\n",
        "    agg_df.to_excel(agg_path, index=False)\n",
        "    print(f\"✅ Saved aggregated data: {agg_path}\\n\")\n",
        "\n",
        "    return agg_df\n",
        "\n"
      ],
      "metadata": {
        "id": "Xy8Xg_SN_rf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 4. DESCRIPTIVE STATISTICS\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "sDtH7FeY_vUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_descriptives(agg_df, task_name):\n",
        "    \"\"\"Calculate descriptive statistics with formatted output.\"\"\"\n",
        "    print(f\"📈 Calculating descriptives for {task_name}...\")\n",
        "\n",
        "    desc = agg_df.groupby(['group', 'emotion', 'item_type']).agg(\n",
        "        n_subjects=('candidate', 'nunique'),\n",
        "        n_trials_total=('n_trials', 'sum'),\n",
        "        mean_hit=('hit_rate', 'mean'),\n",
        "        sd_hit=('hit_rate', 'std'),\n",
        "        mean_false_memory=('false_memory_rate', 'mean'),\n",
        "        sd_false_memory=('false_memory_rate', 'std'),\n",
        "        mean_rt=('rt_mean', 'mean'),\n",
        "        sd_rt=('rt_mean', 'std')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Format as Mean(SD) and add percentages\n",
        "    desc['hit_formatted'] = desc.apply(\n",
        "        lambda r: f\"{r['mean_hit']:.2f} ({r['sd_hit']:.2f}) [{r['mean_hit']*100:.1f}%]\", axis=1\n",
        "    )\n",
        "    desc['false_memory_formatted'] = desc.apply(\n",
        "        lambda r: f\"{r['mean_false_memory']:.2f} ({r['sd_false_memory']:.2f}) [{r['mean_false_memory']*100:.1f}%]\", axis=1\n",
        "    )\n",
        "    desc['rt_formatted'] = desc.apply(\n",
        "        lambda r: f\"{r['mean_rt']:.1f} ({r['sd_rt']:.1f})ms\", axis=1\n",
        "    )\n",
        "\n",
        "    # Save\n",
        "    desc_path = os.path.join(OUTPUT_DIR, f'descriptives_{task_name}.xlsx')\n",
        "    desc.to_excel(desc_path, index=False)\n",
        "\n",
        "    print(f\"✅ Descriptives saved: {desc_path}\")\n",
        "    print(f\"\\n📋 Summary for {task_name.upper()}:\")\n",
        "    print(desc[['group', 'emotion', 'item_type', 'n_subjects', 'false_memory_formatted', 'rt_formatted']].to_string(index=False))\n",
        "    print()\n",
        "\n",
        "    return desc\n"
      ],
      "metadata": {
        "id": "kHE-wqUO_wgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 5. ASSUMPTIONS TESTING\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "xsgOsjlY_09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_assumptions(agg_df, task_name):\n",
        "    \"\"\"Test normality and homogeneity of variance.\"\"\"\n",
        "    print(f\"🔍 Testing assumptions for {task_name}...\")\n",
        "\n",
        "    normality_results = []\n",
        "    homogeneity_results = []\n",
        "\n",
        "    for item in ['Lure', 'Studied']:\n",
        "        for dv in ['false_memory_rate', 'rt_mean']:\n",
        "            # Normality per group/emotion\n",
        "            for group in agg_df['group'].unique():\n",
        "                for emotion in agg_df['emotion'].unique():\n",
        "                    data = agg_df[\n",
        "                        (agg_df['group'] == group) &\n",
        "                        (agg_df['emotion'] == emotion) &\n",
        "                        (agg_df['item_type'] == item)\n",
        "                    ][dv].dropna()\n",
        "\n",
        "                    if len(data) >= 3:\n",
        "                        stat, p = stats.shapiro(data)\n",
        "                        normality_results.append({\n",
        "                            'task': task_name,\n",
        "                            'group': group,\n",
        "                            'emotion': emotion,\n",
        "                            'item_type': item,\n",
        "                            'dv': dv,\n",
        "                            'n': len(data),\n",
        "                            'statistic': stat,\n",
        "                            'p_value': p,\n",
        "                            'normal': p >= ALPHA\n",
        "                        })\n",
        "\n",
        "            # Homogeneity across groups per emotion\n",
        "            for emotion in agg_df['emotion'].unique():\n",
        "                groups_data = []\n",
        "                for group in agg_df['group'].unique():\n",
        "                    data = agg_df[\n",
        "                        (agg_df['group'] == group) &\n",
        "                        (agg_df['emotion'] == emotion) &\n",
        "                        (agg_df['item_type'] == item)\n",
        "                    ][dv].dropna()\n",
        "                    if len(data) >= 2:\n",
        "                        groups_data.append(data)\n",
        "\n",
        "                if len(groups_data) >= 2:\n",
        "                    stat, p = stats.levene(*groups_data)\n",
        "                    homogeneity_results.append({\n",
        "                        'task': task_name,\n",
        "                        'emotion': emotion,\n",
        "                        'item_type': item,\n",
        "                        'dv': dv,\n",
        "                        'statistic': stat,\n",
        "                        'p_value': p,\n",
        "                        'homogeneous': p >= ALPHA\n",
        "                    })\n",
        "\n",
        "    norm_df = pd.DataFrame(normality_results)\n",
        "    homog_df = pd.DataFrame(homogeneity_results)\n",
        "\n",
        "    # Save\n",
        "    norm_df.to_excel(os.path.join(OUTPUT_DIR, f'normality_{task_name}.xlsx'), index=False)\n",
        "    homog_df.to_excel(os.path.join(OUTPUT_DIR, f'homogeneity_{task_name}.xlsx'), index=False)\n",
        "\n",
        "    print(f\"✅ Assumptions tested: {len(norm_df)} normality, {len(homog_df)} homogeneity tests\")\n",
        "    print(f\"   Normal: {norm_df['normal'].sum()}/{len(norm_df)}, Homogeneous: {homog_df['homogeneous'].sum()}/{len(homog_df)}\\n\")\n",
        "\n",
        "    return norm_df, homog_df\n",
        "\n"
      ],
      "metadata": {
        "id": "DWJQUEf6_2oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 6. INFERENTIAL STATISTICS (FIXED)\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "chVDfJlw_5_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mixed_anova(agg_df, task_name, item_type, dv):\n",
        "    \"\"\"Run mixed ANOVA using pingouin (within=emotion, between=group).\"\"\"\n",
        "    print(f\"   🔬 Mixed ANOVA: {task_name}/{item_type}/{dv}\")\n",
        "\n",
        "    data = agg_df[agg_df['item_type'] == item_type].dropna(subset=[dv]).copy()\n",
        "\n",
        "    # Need at least 2 emotions and 2 groups\n",
        "    if data['emotion'].nunique() < 2 or data['group'].nunique() < 2:\n",
        "        print(f\"      ⚠️ Insufficient data (emotions: {data['emotion'].nunique()}, groups: {data['group'].nunique()})\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Pingouin mixed_anova\n",
        "        result = pg.mixed_anova(\n",
        "            data=data,\n",
        "            dv=dv,\n",
        "            within='emotion',\n",
        "            subject='candidate',\n",
        "            between='group',\n",
        "            correction=True  # Greenhouse-Geisser correction\n",
        "        )\n",
        "\n",
        "        result['task'] = task_name\n",
        "        result['item_type'] = item_type\n",
        "        result['dv'] = dv\n",
        "\n",
        "        print(f\"      ✅ Complete\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"      ⚠️ Error: {str(e)[:100]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def run_friedman_test(agg_df, task_name, item_type, dv):\n",
        "    \"\"\"Run Friedman test (non-parametric within-subjects).\"\"\"\n",
        "    print(f\"   📊 Friedman test: {task_name}/{item_type}/{dv}\")\n",
        "\n",
        "    results = []\n",
        "    data = agg_df[agg_df['item_type'] == item_type].dropna(subset=[dv])\n",
        "\n",
        "    for group in data['group'].unique():\n",
        "        group_data = data[data['group'] == group].pivot(\n",
        "            index='candidate',\n",
        "            columns='emotion',\n",
        "            values=dv\n",
        "        ).dropna()\n",
        "\n",
        "        if group_data.shape[1] >= 2 and group_data.shape[0] >= 3:\n",
        "            try:\n",
        "                stat, p = stats.friedmanchisquare(*[group_data[col] for col in group_data.columns])\n",
        "\n",
        "                # Effect size (Kendall's W)\n",
        "                n = group_data.shape[0]\n",
        "                k = group_data.shape[1]\n",
        "                w = stat / (n * (k - 1))\n",
        "\n",
        "                results.append({\n",
        "                    'task': task_name,\n",
        "                    'group': group,\n",
        "                    'item_type': item_type,\n",
        "                    'dv': dv,\n",
        "                    'n': n,\n",
        "                    'chi_square': stat,\n",
        "                    'df': k - 1,\n",
        "                    'p_value': p,\n",
        "                    'kendall_w': w,\n",
        "                    'sig': '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
        "                })\n",
        "                print(f\"      {group}: χ²={stat:.2f}, p={p:.4f}, W={w:.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"      ⚠️ {group}: {str(e)[:50]}\")\n",
        "\n",
        "    return pd.DataFrame(results) if results else None\n",
        "\n",
        "\n",
        "def run_kruskal_wallis(agg_df, task_name, item_type, dv):\n",
        "    \"\"\"Run Kruskal-Wallis test (non-parametric between-groups).\"\"\"\n",
        "    print(f\"   📊 Kruskal-Wallis: {task_name}/{item_type}/{dv}\")\n",
        "\n",
        "    results = []\n",
        "    data = agg_df[agg_df['item_type'] == item_type].dropna(subset=[dv])\n",
        "\n",
        "    for emotion in data['emotion'].unique():\n",
        "        emotion_data = [\n",
        "            data[(data['group'] == g) & (data['emotion'] == emotion)][dv].dropna()\n",
        "            for g in data['group'].unique()\n",
        "        ]\n",
        "\n",
        "        # Check for variance\n",
        "        valid_groups = [g for g in emotion_data if len(g) >= 2 and g.std() > 0]\n",
        "\n",
        "        if len(valid_groups) >= 2:\n",
        "            try:\n",
        "                stat, p = stats.kruskal(*valid_groups)\n",
        "\n",
        "                # Effect size (epsilon squared)\n",
        "                n = sum(len(g) for g in valid_groups)\n",
        "                epsilon2 = (stat - len(valid_groups) + 1) / (n - len(valid_groups))\n",
        "\n",
        "                results.append({\n",
        "                    'task': task_name,\n",
        "                    'emotion': emotion,\n",
        "                    'item_type': item_type,\n",
        "                    'dv': dv,\n",
        "                    'n_groups': len(valid_groups),\n",
        "                    'h_statistic': stat,\n",
        "                    'df': len(valid_groups) - 1,\n",
        "                    'p_value': p,\n",
        "                    'epsilon2': epsilon2,\n",
        "                    'sig': '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'ns'\n",
        "                })\n",
        "                print(f\"      {emotion}: H={stat:.2f}, p={p:.4f}, ε²={epsilon2:.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"      ⚠️ {emotion}: {str(e)[:50]}\")\n",
        "        else:\n",
        "            print(f\"      ⚠️ {emotion}: Insufficient variance or n\")\n",
        "\n",
        "    return pd.DataFrame(results) if results else None\n",
        "\n",
        "\n",
        "def run_posthoc_mann_whitney(agg_df, task_name, item_type, dv, kruskal_results):\n",
        "    \"\"\"Post-hoc Mann-Whitney U tests with Bonferroni correction.\"\"\"\n",
        "    print(f\"   🔬 Post-hoc Mann-Whitney: {task_name}/{item_type}/{dv}\")\n",
        "\n",
        "    results = []\n",
        "    data = agg_df[agg_df['item_type'] == item_type].dropna(subset=[dv])\n",
        "\n",
        "    # Only run for significant Kruskal results\n",
        "    if kruskal_results is None or kruskal_results.empty:\n",
        "        print(\"      ⚠️ No significant Kruskal results\")\n",
        "        return None\n",
        "\n",
        "    sig_emotions = kruskal_results[kruskal_results['p_value'] < ALPHA]['emotion'].unique()\n",
        "\n",
        "    for emotion in sig_emotions:\n",
        "        groups = data['group'].unique()\n",
        "        group_pairs = list(combinations(groups, 2))\n",
        "        n_comparisons = len(group_pairs)\n",
        "        bonferroni_alpha = ALPHA / n_comparisons\n",
        "\n",
        "        for g1, g2 in group_pairs:\n",
        "            data1 = data[(data['group'] == g1) & (data['emotion'] == emotion)][dv].dropna()\n",
        "            data2 = data[(data['group'] == g2) & (data['emotion'] == emotion)][dv].dropna()\n",
        "\n",
        "            if len(data1) >= 2 and len(data2) >= 2:\n",
        "                try:\n",
        "                    stat, p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
        "\n",
        "                    # Effect size (rank-biserial correlation)\n",
        "                    n1, n2 = len(data1), len(data2)\n",
        "                    r = 1 - (2*stat) / (n1 * n2)\n",
        "\n",
        "                    results.append({\n",
        "                        'task': task_name,\n",
        "                        'emotion': emotion,\n",
        "                        'item_type': item_type,\n",
        "                        'dv': dv,\n",
        "                        'group1': g1,\n",
        "                        'group2': g2,\n",
        "                        'n1': n1,\n",
        "                        'n2': n2,\n",
        "                        'u_statistic': stat,\n",
        "                        'p_value': p,\n",
        "                        'p_bonferroni': p * n_comparisons,\n",
        "                        'bonferroni_alpha': bonferroni_alpha,\n",
        "                        'r_effect': r,\n",
        "                        'sig_corrected': p < bonferroni_alpha\n",
        "                    })\n",
        "                    print(f\"      {g1} vs {g2} ({emotion}): U={stat:.1f}, p={p:.4f}, r={r:.3f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"      ⚠️ {g1} vs {g2}: {str(e)[:50]}\")\n",
        "\n",
        "    return pd.DataFrame(results) if results else None\n",
        "\n",
        "\n",
        "def run_statistical_tests(agg_df, task_name):\n",
        "    \"\"\"Run all statistical tests for a task.\"\"\"\n",
        "    print(f\"\\n🧪 Running statistical tests for {task_name}...\\n\")\n",
        "\n",
        "    all_anova = []\n",
        "    all_friedman = []\n",
        "    all_kruskal = []\n",
        "    all_posthoc = []\n",
        "\n",
        "    for item in ['Lure', 'Studied']:\n",
        "        for dv in ['false_memory_rate', 'rt_mean']:\n",
        "            print(f\"\\n--- {item} / {dv} ---\")\n",
        "\n",
        "            # Try mixed ANOVA first\n",
        "            anova_result = run_mixed_anova(agg_df, task_name, item, dv)\n",
        "            if anova_result is not None:\n",
        "                all_anova.append(anova_result)\n",
        "\n",
        "            # Non-parametric alternatives\n",
        "            friedman_result = run_friedman_test(agg_df, task_name, item, dv)\n",
        "            if friedman_result is not None:\n",
        "                all_friedman.append(friedman_result)\n",
        "\n",
        "            kruskal_result = run_kruskal_wallis(agg_df, task_name, item, dv)\n",
        "            if kruskal_result is not None:\n",
        "                all_kruskal.append(kruskal_result)\n",
        "\n",
        "                # Post-hoc if Kruskal is significant\n",
        "                posthoc_result = run_posthoc_mann_whitney(agg_df, task_name, item, dv, kruskal_result)\n",
        "                if posthoc_result is not None:\n",
        "                    all_posthoc.append(posthoc_result)\n",
        "\n",
        "    # Save all results\n",
        "    if all_anova:\n",
        "        pd.concat(all_anova).to_excel(os.path.join(OUTPUT_DIR, f'anova_{task_name}.xlsx'), index=False)\n",
        "    if all_friedman:\n",
        "        pd.concat(all_friedman).to_excel(os.path.join(OUTPUT_DIR, f'friedman_{task_name}.xlsx'), index=False)\n",
        "    if all_kruskal:\n",
        "        pd.concat(all_kruskal).to_excel(os.path.join(OUTPUT_DIR, f'kruskal_{task_name}.xlsx'), index=False)\n",
        "    if all_posthoc:\n",
        "        pd.concat(all_posthoc).to_excel(os.path.join(OUTPUT_DIR, f'posthoc_{task_name}.xlsx'), index=False)\n",
        "\n",
        "    print(f\"\\n✅ Statistical tests complete for {task_name}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "k6MvRBPS_5tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 7. POWER ANALYSIS\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "-cbmn51kABGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_power_analysis(agg_df, task_name):\n",
        "    \"\"\"Calculate statistical power for ANOVA designs.\"\"\"\n",
        "    print(f\"⚡ Calculating power analysis for {task_name}...\")\n",
        "\n",
        "    power_calc = FTestAnovaPower()\n",
        "    results = []\n",
        "\n",
        "    for item in ['Lure', 'Studied']:\n",
        "        for dv in ['false_memory_rate', 'rt_mean']:\n",
        "            data = agg_df[agg_df['item_type'] == item].dropna(subset=[dv])\n",
        "\n",
        "            n_groups = data['group'].nunique()\n",
        "            n_per_group = data.groupby('group')['candidate'].nunique().mean()\n",
        "\n",
        "            if n_groups >= 2 and n_per_group >= 2:\n",
        "                # Calculate observed power\n",
        "                power_observed = power_calc.solve_power(\n",
        "                    effect_size=EFFECT_SIZE_MEDIUM,\n",
        "                    nobs=n_per_group * n_groups,\n",
        "                    alpha=ALPHA,\n",
        "                    k_groups=n_groups\n",
        "                )\n",
        "\n",
        "                # Calculate needed n for 0.80 power\n",
        "                n_needed = power_calc.solve_power(\n",
        "                    effect_size=EFFECT_SIZE_MEDIUM,\n",
        "                    power=0.80,\n",
        "                    alpha=ALPHA,\n",
        "                    k_groups=n_groups\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    'task': task_name,\n",
        "                    'item_type': item,\n",
        "                    'dv': dv,\n",
        "                    'n_groups': n_groups,\n",
        "                    'n_per_group': int(n_per_group),\n",
        "                    'total_n': int(n_per_group * n_groups),\n",
        "                    'effect_size': EFFECT_SIZE_MEDIUM,\n",
        "                    'alpha': ALPHA,\n",
        "                    'power_observed': power_observed,\n",
        "                    'n_needed_0.8_power': int(np.ceil(n_needed)),\n",
        "                    'adequately_powered': power_observed >= 0.80\n",
        "                })\n",
        "\n",
        "    power_df = pd.DataFrame(results)\n",
        "    power_df.to_excel(os.path.join(OUTPUT_DIR, f'power_analysis_{task_name}.xlsx'), index=False)\n",
        "\n",
        "    print(f\"✅ Power analysis complete\")\n",
        "    print(power_df.to_string(index=False))\n",
        "    print()\n",
        "\n",
        "    return power_df\n",
        "\n"
      ],
      "metadata": {
        "id": "G51OfmG_AC5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================\n",
        "# 8. VISUALIZATION\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "id": "joB-7z0FAFxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_visualizations(agg_df, task_name, desc_df):\n",
        "    \"\"\"Create publication-quality plots.\"\"\"\n",
        "    print(f\"📊 Creating visualizations for {task_name}...\")\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.rcParams['figure.dpi'] = 300\n",
        "\n",
        "    for item in ['Lure', 'Studied']:\n",
        "        data = agg_df[agg_df['item_type'] == item]\n",
        "\n",
        "        # False memory rate bar plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.barplot(data=data, x='emotion', y='false_memory_rate', hue='group',\n",
        "                   errorbar='sd', palette='Set2', ax=ax)\n",
        "        ax.set_title(f'False Memory Rate by Emotion and Group - {item} ({task_name})',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('False Memory Rate', fontsize=12)\n",
        "        ax.set_xlabel('Emotion Type', fontsize=12)\n",
        "        ax.legend(title='Group', fontsize=10)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.1f%%', label_type='edge',\n",
        "                        labels=[f'{h.get_height()*100:.1f}%' for h in container])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f'bar_false_memory_{item}_{task_name}."
      ],
      "metadata": {
        "id": "HI7_v5X4AHJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gl4_dEqHBnnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}